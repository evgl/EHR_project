{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction\n",
    "Extract data from MIMIC-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import re\n",
    "\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.plotly as py\n",
    "import cufflinks\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# import plotly.figure_factory as ff\n",
    "# InteractiveShell.ast_node_interactivity = 'all'\n",
    "# from plotly.offline import iplot\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd \n",
    "\n",
    "start = time.time()\n",
    "# reading csv file \n",
    "admissions_df = pd.read_csv(\"../MIMIC-III/ADMISSIONS.csv\")\n",
    "noteevents_df = pd.read_csv(\"../MIMIC-III/NOTEEVENTS.csv\") \n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnosis = df.DIAGNOSIS.unique()\n",
    "# #PNEUMONIA\n",
    "# for i in diagnosis:\n",
    "#     print(f\"[{i}]\")\n",
    "# data = df.copy()\n",
    "# data.info()\n",
    "\n",
    "# pneumonia_df = data.loc[df['DIAGNOSIS'] == 'PNEUMONIA']\n",
    "# pneumonia_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "admissions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noteevents_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/left-join-with-pandas-data-frames-in-python-c29c85089ba4\n",
    "# left_join = noteevents_df.join(admissions_df.set_index('HADM_ID'), on='HADM_ID')\n",
    "# left_join = noteevents_df.set_index('HADM_ID').join(admissions_df.set_index('HADM_ID'))\n",
    "\"\"\"\n",
    ">>> left\n",
    "  transaction_id user_id     value\n",
    "0              A   Peter  1.867558\n",
    "1              B    John -0.977278\n",
    "2              C    John  0.950088\n",
    "3              D    Anna -0.151357\n",
    ">>> right\n",
    "  user_id favorite_color\n",
    "0    Paul           blue\n",
    "1    Mary           blue\n",
    "2    John            red\n",
    "3    Anna            NaN\n",
    "\n",
    "---------\n",
    ">>> left.merge(right, on='user_id', how='left', indicator=True)\n",
    "  transaction_id user_id     value favorite_color     _merge\n",
    "0              A   Peter  1.867558            NaN  left_only\n",
    "1              B    John -0.977278            red       both\n",
    "2              C    John  0.950088            red       both\n",
    "3              D    Anna -0.151357            NaN       both\n",
    "\n",
    "---------\n",
    "left.merge(right.rename({'user_id': 'user_id_r'}, axis=1),\n",
    "               left_on='user_id', right_on='user_id_r', how='left')\n",
    "               \n",
    "  transaction_id user_id     value user_id_r favorite_color\n",
    "0              A   Peter  1.867558       NaN            NaN\n",
    "1              B    John -0.977278      John            red\n",
    "2              C    John  0.950088      John            red\n",
    "3              D    Anna -0.151357      Anna            NaN\n",
    "---------\n",
    "select\n",
    "    t.transaction_id\n",
    "    , t.user_id\n",
    "    , t.value\n",
    "    , u.user_id as user_id_r\n",
    "    , u.favorite_color\n",
    "from\n",
    "    transactions t\n",
    "    left join\n",
    "    users u\n",
    "    on t.user_id = u.user_id\n",
    ";\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Left join of two dataframes\n",
    "note_admiss_df_left = noteevents_df.merge(admissions_df, on='HADM_ID', how='left', indicator=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "note_admiss_df_left.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_admiss_df_left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df1 = df[['a','b']]\n",
    "note_admiss_df_left[['DIAGNOSIS', 'SUBJECT_ID_x', 'SUBJECT_ID_y','DESCRIPTION', 'CATEGORY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pneumonia_df = note_admiss_df_left.loc[note_admiss_df_left[\"DIAGNOSIS\"] == 'PNEUMONIA', ['ROW_ID_x', 'SUBJECT_ID_x', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'DIAGNOSIS', 'HAS_CHARTEVENTS_DATA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pneumonia_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pneumonia_df['CATEGORY'].value_counts().sort_values(ascending=False).iplot(kind='bar', yTitle='Number of notes', \n",
    "                                                                title='Number of category kinds in noteevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pneumonia_no_disch_df = pneumonia_df.loc[pneumonia_df[\"CATEGORY\"] != 'Discharge summary', ['ROW_ID_x', 'SUBJECT_ID_x', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'DIAGNOSIS', 'HAS_CHARTEVENTS_DATA']]\n",
    "pneumonia_no_disch_df = pneumonia_df.loc[pneumonia_df[\"CATEGORY\"] != 'Discharge summary', ['ROW_ID_x','SUBJECT_ID_x','CHARTDATE','STORETIME','CATEGORY','DESCRIPTION','TEXT', 'DEATHTIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pneumonia_no_disch_df['CATEGORY'].value_counts().sort_values(ascending=False).iplot(kind='bar', yTitle='Number of notes', \n",
    "                                                                title='Number of category kinds in noteevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_no_disch_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_no_disch_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch = pneumonia_no_disch_df[pneumonia_no_disch_df.DEATHTIME.isnull()]\n",
    "pneumonia_dead_no_disch = pneumonia_no_disch_df[pneumonia_no_disch_df.DEATHTIME.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_dead_no_disch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch = pneumonia_alive_no_disch.sort_values(by=['SUBJECT_ID_x','CHARTDATE', 'ROW_ID_x'])\n",
    "pneumonia_dead_no_disch = pneumonia_dead_no_disch.sort_values(by=['SUBJECT_ID_x','CHARTDATE', 'ROW_ID_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch['CATEGORY'].value_counts().sort_values(ascending=False).iplot(kind='bar', yTitle='Number of notes', \n",
    "                                                                title='Number of category kinds in pneumonia alive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pneumonia_dead_no_disch['CATEGORY'].value_counts().sort_values(ascending=False).iplot(kind='bar', yTitle='Number of notes', \n",
    "                                                                title='Number of category kinds in pneumonia alive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patients\n",
    "pneumonia_dead_no_disch['SUBJECT_ID_x'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of patients\n",
    "pneumonia_alive_no_disch['SUBJECT_ID_x'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text handling tool\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "# define constants\n",
    "# RESULT_HEADER = \"WORD, FREQUENCY\\n\"\n",
    "MIN_SEQ_LEN = 4\n",
    "USE_1_N_SEQ = 2\n",
    "\n",
    "\n",
    "# words that do not have meaning (can be modified later)\n",
    "USELESS_WORDS = [\"a\", \"the\", \"he\", \"she\", \",\", \".\", \"?\", \"!\", \":\", \";\", \"+\", \"*\", \"**\"\\\n",
    "                 \"your\", \"you\"]\n",
    "\n",
    "# count up the frequency of every word in every disease file\n",
    "stemmer = PorterStemmer()\n",
    "# create set of words to ignore in text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for word in USELESS_WORDS:\n",
    "    stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------\n",
    "def count_notes_per_patient(disease_df):\n",
    "    patient_id_to_num_notes = {}\n",
    "    patient_id = -1\n",
    "    note_counter = 0\n",
    "            \n",
    "    for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "        patient_id_check = int(row['SUBJECT_ID_x'])\n",
    "                \n",
    "        if not patient_id == patient_id_check:\n",
    "            patient_id_to_num_notes[patient_id] = note_counter\n",
    "            note_counter = 1\n",
    "        else:\n",
    "            note_counter += 1\n",
    "                    \n",
    "        patient_id = patient_id_check\n",
    "                \n",
    "    patient_id_to_num_notes[patient_id] = note_counter\n",
    "    del patient_id_to_num_notes[-1]\n",
    "    return patient_id_to_num_notes\n",
    "\n",
    "patient_id_to_num_notes = {}\n",
    "patient_id_to_num_notes['pneumonia_dead'] = count_notes_per_patient(pneumonia_dead_no_disch)\n",
    "patient_id_to_num_notes['pneumonia_alive'] = count_notes_per_patient(pneumonia_alive_no_disch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id_to_num_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(patient_id_to_num_notes['pneumonia_dead']))\n",
    "print(len(patient_id_to_num_notes['pneumonia_alive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pneumonia_dead_no_disch.loc[pneumonia_dead_no_disch[\"SUBJECT_ID_x\"] == 111, ['ROW_ID_x','SUBJECT_ID_x','DESCRIPTION','TEXT', 'DEATHTIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp_df = pneumonia_dead_no_disch.loc[pneumonia_dead_no_disch[\"SUBJECT_ID_x\"] == 111, ['DESCRIPTION','TEXT']]\n",
    "# for index, row in tmp_df.iterrows():\n",
    "#     print(row['DESCRIPTION'])\n",
    "#     print(row['TEXT'])\n",
    "#     print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# row_cnt = 0\n",
    "# note_cnt = 0\n",
    "# for index, row in pneumonia_alive_no_disch.iterrows():\n",
    "#     row_cnt += 1\n",
    "#     note = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|[_,\\d\\*:~=\\.\\-\\+\\\\/]+', ' ', row['TEXT'])\n",
    "#     tokenized_note = word_tokenize(note)\n",
    "       \n",
    "#     if 'money' in tokenized_note :\n",
    "#         note_cnt += 1\n",
    "#         print(f\"#{row_cnt}) {note_cnt}) {row['ROW_ID_x']} : {row['SUBJECT_ID_x']} ==> [{row['DESCRIPTION']}]\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# row_cnt = 0\n",
    "# note_cnt = 0\n",
    "# for index, row in pneumonia_alive_no_disch.iterrows():\n",
    "#     row_cnt += 1\n",
    "#     note = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|[_,\\d\\*:~=\\.\\-\\+\\\\/]+', ' ', row['TEXT'])\n",
    "#     tokenized_note = word_tokenize(note)\n",
    "    \n",
    "#     if 'money' in tokenized_note and 'wallet' in tokenized_note:\n",
    "#         note_cnt += 1\n",
    "#         print(f\"#{row_cnt}) {note_cnt}) {row['ROW_ID_x']} : {row['SUBJECT_ID_x']} ==> [{row['DESCRIPTION']}]\")\n",
    "# #         print(row['TEXT'])\n",
    "# #         print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_per_patient(disease_df, patient_id_to_num_notes):\n",
    "    note_appearance_counter = {}\n",
    "    number_of_patients = 0 # number of patients\n",
    "    note_counter = 0\n",
    "\n",
    "# -----------\n",
    "    patient_id = -1\n",
    "    word_set = set()\n",
    "    note_event_counter = 0\n",
    "\n",
    "    # Iterate through each note\n",
    "    for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "\n",
    "        \n",
    "        patient_id_check = int(row['SUBJECT_ID_x'])\n",
    "    \n",
    "        # if patient id has changed, end sequence and start new sequence\n",
    "        if not patient_id == patient_id_check:\n",
    "            number_of_patients += 1\n",
    "            note_event_counter = 0\n",
    "        \n",
    "            for word in word_set:\n",
    "                if word in note_appearance_counter:\n",
    "                    note_appearance_counter[word] += 1\n",
    "                else:\n",
    "                    note_appearance_counter[word] = 1\n",
    "\n",
    "        \n",
    "            # reset word_set\n",
    "            word_set = set()\n",
    "        \n",
    "        # update patient id\n",
    "        patient_id = patient_id_check\n",
    "\n",
    "            \n",
    "        if patient_id_to_num_notes[patient_id_check] <= MIN_SEQ_LEN:\n",
    "            continue\n",
    "            \n",
    "        if note_event_counter < patient_id_to_num_notes[patient_id] // USE_1_N_SEQ:\n",
    "            note_event_counter += 1\n",
    "            continue\n",
    "                \n",
    "        note_counter += 1\n",
    "        note = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|[_,\\d\\*:~=\\.\\-\\+\\\\/]+', ' ', row['TEXT'])\n",
    "        tokenized_note = word_tokenize(note)\n",
    "        \n",
    "        \n",
    "        for word in tokenized_note:\n",
    "            stemmed_word = stemmer.stem(word.lower())\n",
    "            if not stemmed_word in stop_words:\n",
    "                word_set.add(stemmed_word)\n",
    "    \n",
    "    print(str(note_counter) + \" note events\")\n",
    "    print(\"finished counting frequent words for patients!\")\n",
    "#     return note_counter, note_appearance_counter\n",
    "    return number_of_patients, note_appearance_counter\n",
    "\n",
    "# variable dictionaries\n",
    "number_of_notes = {}\n",
    "note_appearance_counter = {}\n",
    "\n",
    "number_of_notes['pneumonia_dead'], note_appearance_counter['pneumonia_dead'] = count_words_per_patient(pneumonia_dead_no_disch, patient_id_to_num_notes['pneumonia_dead'])\n",
    "number_of_notes['pneumonia_alive'], note_appearance_counter['pneumonia_alive'] = count_words_per_patient(pneumonia_alive_no_disch, patient_id_to_num_notes['pneumonia_alive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(note_appearance_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get hyper-paramters n_fold and threshold from user input\n",
    "n_fold = float(3)\n",
    "threshold = float(0.02)\n",
    "\n",
    "frequent_word_lists = {}\n",
    "factor = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function description:\n",
    "for each disease in note_appearance_counter\n",
    "    1. checks whether a word in disease file is frequent(frequency standard as defined by factor, n_fold, and threshold)\n",
    "    2. adds to frequent_word_list\n",
    "\"\"\"\n",
    "\n",
    "# def find_frequent_word(note_appearance_counter, frequent_word_lists, number_of_notes, factor, n_fold, threshold):\n",
    "\n",
    "# calculate normalizing factor for each disease\n",
    "note_sum = 0\n",
    "\n",
    "# Count from two labels\n",
    "for disease in number_of_notes:\n",
    "    note_sum += float(number_of_notes[disease])\n",
    "    \n",
    "for disease in number_of_notes:\n",
    "    factor[disease] = number_of_notes[disease] / note_sum\n",
    "\n",
    "# determine frequent word for each disease file\n",
    "for disease in note_appearance_counter:\n",
    "    frequent_word_lists[disease] = []\n",
    "\n",
    "    print(disease + \" has \" + str(len(note_appearance_counter[disease])) + \" unique words!\")\n",
    "\n",
    "    for word in note_appearance_counter[disease]:\n",
    "        \n",
    "        freq_check = True\n",
    "        for check_disease in note_appearance_counter:\n",
    "            \n",
    "            if not disease == check_disease:\n",
    "                if word in note_appearance_counter[check_disease]:\n",
    "                    if not (note_appearance_counter[disease][word] / note_appearance_counter[check_disease][word] / factor[disease] * factor[check_disease] > n_fold \\\n",
    "                        and note_appearance_counter[disease][word] > (number_of_notes[disease] * threshold)):\n",
    "\n",
    "                        freq_check = False\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    if not (note_appearance_counter[disease][word] > n_fold and note_appearance_counter[disease][word] > (number_of_notes[disease] * threshold)):\n",
    "                        freq_check = False\n",
    "                        break\n",
    "        if freq_check:\n",
    "            frequent_word_lists[disease].append((word))\n",
    "            # Create a tuple of word and its count\n",
    "#             frequent_word_lists[disease].append((word, note_appearance_counter[disease][word]))\n",
    "\n",
    "        \n",
    "\n",
    "print(\"finished making frequent words list for \" + disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(note_sum)\n",
    "print(factor)\n",
    "\n",
    "# number of notes\n",
    "# 58029.0\n",
    "# {'pneumonia_dead': 0.2049837150390322, 'pneumonia_alive': 0.7950162849609678}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(frequent_word_lists['pneumonia_dead']))\n",
    "print(len(frequent_word_lists['pneumonia_alive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(frequent_word_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Co-occurrence generation\n",
    "FP-growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENT_WORD_LIST = frequent_word_lists['pneumonia_dead'] + frequent_word_lists['pneumonia_alive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"function description:\n",
    "generates frequent word set for the disease\n",
    "\"\"\"\n",
    "word_dict = {}\n",
    "word_id = 1\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "for word in FREQUENT_WORD_LIST:\n",
    "    if not word == \"WORD\":\n",
    "        word_dict[stemmer.stem(word.strip())] = word_id\n",
    "        word_id += 1\n",
    "             \n",
    "print(\"\\nword dictionary created!\\n\")\n",
    "print(word_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementetion with python for FP_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/\n",
    "import os\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# ----\n",
    "def fp_growth_input_per_patient_python(disease_name, disease_df, word_dict, min_support, label):\n",
    "    \n",
    "    # For dataframe\n",
    "    patient_id_lst = []\n",
    "    patient_cooc_lst = []\n",
    "    patient_cooc_minsup_lst = []\n",
    "    patient_label_lst = []\n",
    "    patient_note_cnt = []\n",
    "    \n",
    "    # --------------    \n",
    "    patient_id = -1\n",
    "    note_cnt = 0\n",
    "    patient_note_list = []\n",
    "    \n",
    "    # read line in from file (each line is one note)\n",
    "    for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "        \n",
    "        # only regard certain type of notes\n",
    "        patient_id_check = int(row['SUBJECT_ID_x'])\n",
    "        note = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|[_,\\d\\*:~=\\.\\-\\+\\\\/]+', ' ', row['TEXT'])\n",
    "        patient_word_set = set()\n",
    "    \n",
    "#         print(f\"patient_id_check: {patient_id_check}, patient_id: {patient_id}\")\n",
    "        # if patient id has changed, end sequence and start new sequence\n",
    "        if not patient_id == patient_id_check and not patient_id == -1:\n",
    "            te = TransactionEncoder()\n",
    "            te_ary = te.fit(patient_note_list).transform(patient_note_list)\n",
    "            df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "            df_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "            \n",
    "            cooc_tmp = []\n",
    "            cooc_minsup_tmp = []\n",
    "            \n",
    "            for index, row in df_itemsets.iterrows():\n",
    "                if len(row['itemsets']) == 2:\n",
    "                    cooc_tmp.append(list(row['itemsets']))\n",
    "                    cooc_minsup_tmp.append(row['support'])\n",
    "                    \n",
    "            \n",
    "            # Update glob lists\n",
    "            patient_id_lst.append(patient_id)\n",
    "            patient_cooc_lst.append(cooc_tmp)\n",
    "            patient_cooc_minsup_lst.append(cooc_minsup_tmp)\n",
    "            patient_label_lst.append(label)\n",
    "            patient_note_cnt.append(note_cnt)\n",
    "            \n",
    "            # Reset local lists\n",
    "            patient_note_list = []\n",
    "            note_cnt = 0\n",
    "                    \n",
    "        # update patient id\n",
    "        patient_id = patient_id_check\n",
    "        tokenized_note = word_tokenize(note)\n",
    "        note_cnt += 1\n",
    "\n",
    "        # loop through each word in note to count word belonging to each disease\n",
    "        for word in tokenized_note:\n",
    "            stemmed_word = stemmer.stem(word.lower())       \n",
    "            if stemmed_word in word_dict:\n",
    "                    patient_word_set.add(stemmed_word)\n",
    "\n",
    "        templst = []\n",
    "        for word in patient_word_set:\n",
    "            templst.append(word)\n",
    "\n",
    "        if templst:\n",
    "            patient_note_list.append(templst)\n",
    "    \n",
    "    # Last patient info\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(patient_note_list).transform(patient_note_list)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    df_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "            \n",
    "    cooc_tmp = []\n",
    "    cooc_minsup_tmp = []\n",
    "            \n",
    "    for index, row in df_itemsets.iterrows():\n",
    "        if len(row['itemsets']) == 2:\n",
    "            cooc_tmp.append(row['itemsets'])\n",
    "            cooc_minsup_tmp.append(row['support'])\n",
    "                    \n",
    "            \n",
    "    # Update glob lists\n",
    "    patient_id_lst.append(patient_id)\n",
    "    patient_cooc_lst.append(cooc_tmp)\n",
    "    patient_cooc_minsup_lst.append(cooc_minsup_tmp)\n",
    "    patient_label_lst.append(label)\n",
    "    patient_note_cnt.append(note_cnt)\n",
    "    \n",
    "#     print(f\"patient_id_lst: {len(patient_id_lst)} patient_cooc_lst: {len(patient_cooc_lst)} patient_cooc_minsup_lst: {len(patient_cooc_minsup_lst)} patient_label_lst: {len(patient_label_lst)} patient_note_cnt: {len(patient_note_cnt)}\")\n",
    "    print(\"FP growth input for \" + disease_name + \" generated!\")\n",
    "    return pd.DataFrame({\"patient_id\":patient_id_lst, \"patient_cooc\":patient_cooc_lst, \"cooc_minsup\":patient_cooc_minsup_lst, \"label\":patient_label_lst, \"note_cnt\":patient_note_cnt})\n",
    "\n",
    "pneumonia_dead_cooc_df = fp_growth_input_per_patient_python(\"pneumonia_dead\", pneumonia_dead_no_disch, word_dict, 0.4, -1)\n",
    "pneumonia_alive_cooc_df = fp_growth_input_per_patient_python(\"pneumonia_alive\", pneumonia_alive_no_disch, word_dict, 0.4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pneumonia_dead_cooc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_ = 0\n",
    "for i, (index, row) in enumerate(pneumonia_dead_cooc_df.iterrows()):\n",
    "    if row['patient_cooc']:\n",
    "        cnt_ += 1\n",
    "print(cnt_)\n",
    "# 230 min_sup = 0.1\n",
    "# 74 min_sup = 0.4\n",
    "# 94 min_sup = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_cooc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_ = 0\n",
    "for i, (index, row) in enumerate(pneumonia_alive_cooc_df.iterrows()):\n",
    "    if row['patient_cooc']:\n",
    "        cnt_ += 1\n",
    "print(cnt_)\n",
    "# 763 min_sup = 0.1\n",
    "# 230 min_sup = 0.4\n",
    "# 381 min_sup = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import GCNSupervisedGraphClassification\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from stellargraph import datasets\n",
    "\n",
    "from sklearn import model_selection\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@kegui/how-do-i-know-i-am-running-keras-model-on-gpu-a9cdcc24f986\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph import StellarGraph\n",
    "\n",
    "def create_graph_list(pd_df):\n",
    "    graphs = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in pd_df.iterrows():\n",
    "        if row['patient_cooc']:\n",
    "            source = []\n",
    "            target = []\n",
    "            for cooc in row['patient_cooc']:\n",
    "                source.append(cooc[0])\n",
    "                target.append(cooc[1])\n",
    "            \n",
    "            square_edges = pd.DataFrame({\"source\": source, \"target\": target})\n",
    "            square = StellarGraph(edges=square_edges)\n",
    "            graphs.append(square)\n",
    "            labels.append(row['label'])\n",
    "    return graphs, labels\n",
    "\n",
    "graphs = []\n",
    "labels = []\n",
    "\n",
    "graph_1, label_1 = create_graph_list(pneumonia_dead_cooc_df)\n",
    "graph_2, label_2= create_graph_list(pneumonia_alive_cooc_df)\n",
    "\n",
    "graphs.extend(graph_1)\n",
    "labels.extend(label_1)\n",
    "print(f\"graphs: {len(graphs)}, labels: {len(graphs)}\")\n",
    "graphs.extend(graph_2)\n",
    "labels.extend(label_2)\n",
    "print(f\"graphs: {len(graphs)}, labels: {len(graphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation for spmf java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\"\"\"function description:\n",
    "generates patient word itemset for each disease\n",
    "\"\"\"\n",
    "# # define stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "def generate_FP_input_per_note(disease_name, disease_df, word_dict):\n",
    "    with open(disease_name, \"w\") as output_fp:\n",
    "\n",
    "        # write the coverted dic for the text\n",
    "        # od = collections.OrderedDict(sorted(word_dict.items()))\n",
    "        od = sorted(word_dict.items(), key = operator.itemgetter(1))\n",
    "        output_fp.write(\"@CONVERTED_FROM_TEXT\")\n",
    "        output_fp.write(\"\\n\")\n",
    "        # for k, v in od.iteritems():\n",
    "        for k,v in od:\n",
    "            output_fp.write(\"@ITEM=\")\n",
    "            output_fp.write(str(v))\n",
    "            output_fp.write(\"=\")\n",
    "            output_fp.write(str(k))\n",
    "            output_fp.write(\"\\n\")\n",
    "\n",
    "        # read line in from file (each line is one note)\n",
    "        for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "            # if not \"Physician\" in line[4]:\n",
    "            #     continue\n",
    "                    \n",
    "            word_set = set()\n",
    "            note = row['TEXT']\n",
    "\n",
    "            tokenized_note = word_tokenize(note)\n",
    "\n",
    "            # loop through each word in note to count word belonging to each disease\n",
    "            for word in tokenized_note:\n",
    "                stemmed_word = stemmer.stem(word.lower())\n",
    "                if stemmed_word in word_dict:\n",
    "                    word_set.add(stemmed_word)\n",
    "\n",
    "            templst = []\n",
    "            for word in word_set:\n",
    "                templst.append(word_dict[word])\n",
    "\n",
    "            templst.sort()\n",
    "            for x in range(len(templst) - 1):\n",
    "                output_fp.write(str(templst[x]) + \" \")\n",
    "\n",
    "            if templst:\n",
    "                output_fp.write(str(templst[len(templst)-1]))\n",
    "                output_fp.write(\"\\n\")\n",
    "            \n",
    "    print(\"FP growth input for \" + disease_name + \" generated!\")\n",
    "\n",
    "# Given as a parameter\n",
    "generate_FP_input_per_note(\"pneumonia_dead\", pneumonia_dead_no_disch, word_dict)\n",
    "generate_FP_input_per_note(\"pneumonia_alive\", pneumonia_alive_no_disch, word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "from pathlib import Path\n",
    "\n",
    "# ----\n",
    "def fp_growth_input_per_patient(disease_name, disease_df, word_dict):\n",
    "    \n",
    "    disease_path = os.path.join(\"./tmp\", disease_name, \"inputs\")\n",
    "    if not os.path.exists(disease_path):\n",
    "        os.makedirs(disease_path)\n",
    "        \n",
    "    patient_id = -1\n",
    "#     patient_word_set = set()\n",
    "    patient_note_list = []\n",
    "    \n",
    "    # read line in from file (each line is one note)\n",
    "    for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "        # only regard certain type of notes\n",
    "        patient_id_check = int(row['SUBJECT_ID_x'])\n",
    "        note = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|[_,\\d\\*:~=\\.\\-\\+\\\\/]+', ' ', row['TEXT'])\n",
    "        patient_word_set = set()\n",
    "    \n",
    "#         print(f\"patient_id_check: {patient_id_check}, patient_id: {patient_id}\")\n",
    "        # if patient id has changed, end sequence and start new sequence\n",
    "        if not patient_id == patient_id_check and not patient_id == -1:\n",
    "        \n",
    "#             disease_path = \"./tmp/\" + disease_name\n",
    "            file_name = str(patient_id) + '_' + disease_name\n",
    "            file_path = os.path.join(disease_path, file_name)\n",
    "            \n",
    "            with open(file_path, \"w\") as output_fp:\n",
    "                # write the coverted dic for the text\n",
    "                # od = collections.OrderedDict(sorted(word_dict.items()))\n",
    "    \n",
    "                od = sorted(word_dict.items(), key = operator.itemgetter(1))\n",
    "                output_fp.write(\"@CONVERTED_FROM_TEXT\")\n",
    "                output_fp.write(\"\\n\")\n",
    "                # for k, v in od.iteritems():\n",
    "                for k,v in od:\n",
    "                    output_fp.write(\"@ITEM=\")\n",
    "                    output_fp.write(str(v))\n",
    "                    output_fp.write(\"=\")\n",
    "                    output_fp.write(str(k))\n",
    "                    output_fp.write(\"\\n\")\n",
    "\n",
    "                for item in patient_note_list:\n",
    "                    for x in range(len(item) - 1):\n",
    "                        output_fp.write(str(item[x]) + \" \")\n",
    "\n",
    "                    if item:\n",
    "                        output_fp.write(str(item[len(item)-1]))\n",
    "                        output_fp.write(\"\\n\")\n",
    "\n",
    "                patient_note_list = []\n",
    "                \n",
    "\n",
    "                    \n",
    "        # update patient id\n",
    "        patient_id = patient_id_check\n",
    "        tokenized_note = word_tokenize(note)\n",
    "\n",
    "        # loop through each word in note to count word belonging to each disease\n",
    "        for word in tokenized_note:\n",
    "            stemmed_word = stemmer.stem(word.lower())       \n",
    "            if stemmed_word in word_dict:\n",
    "                    patient_word_set.add(stemmed_word)\n",
    "\n",
    "        templst = []\n",
    "        for word in patient_word_set:\n",
    "            templst.append(word_dict[word])\n",
    "\n",
    "        templst.sort()\n",
    "        patient_note_list.append(templst)\n",
    "        \n",
    "            \n",
    "    print(\"FP growth input for \" + disease_name + \" generated!\")\n",
    "\n",
    "fp_growth_input_per_patient(\"pneumonia_dead\", pneumonia_dead_no_disch, word_dict)\n",
    "fp_growth_input_per_patient(\"pneumonia_alive\", pneumonia_alive_no_disch, word_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "print(os.getcwd())\n",
    "os.system('ls -l')\n",
    "# process = subprocess.Popen([\"java -jar spmf run FPGrowth_itemsets pneumonia_dead FP_out_test 3%\"])\n",
    "# process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "from tqdm import tqdm\n",
    "def fp_growth_output_per_pratient(disease_name):\n",
    "#     spmf = ['java', '-jar', 'spmf.jar', 'run', 'FPGrowth_itemsets']\n",
    "    minsup = '70%'\n",
    "    inputdir = os.path.join(\"./tmp\", disease_name, \"inputs\")\n",
    "    \n",
    "    outputdir = os.path.join(\"./tmp\", disease_name, \"outputs\")\n",
    "    if not os.path.exists(outputdir):\n",
    "        os.makedirs(outputdir)\n",
    "        \n",
    "    for patient_file in tqdm(os.listdir(inputdir)):\n",
    "#         print(patient_file)\n",
    "        input_ = os.path.join(inputdir, patient_file)\n",
    "        output_ = os.path.join(outputdir, patient_file)\n",
    "#         print(f\"input_: {input_}\")\n",
    "#         print(f\"output_: {output_}\")\n",
    "        command = ['java', '-jar', 'spmf.jar', 'run', 'FPGrowth_itemsets', input_, output_, minsup]\n",
    "#         print(f\"command: {command}\")\n",
    "        subprocess.call(command)\n",
    "\n",
    "        \n",
    "\n",
    "fp_growth_output_per_pratient(\"pneumonia_dead\")\n",
    "fp_growth_output_per_pratient(\"pneumonia_alive\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findItemSets(fileName, length_OfItemSet):\n",
    "\n",
    "    file_read = fileName\n",
    "    lengthOfItemSet = int(length_OfItemSet)\n",
    "\n",
    "    wordlst = []\n",
    "    with open(file_read, \"r\") as fp:\n",
    "        for line in fp:\n",
    "            line = line.split()\n",
    "            if line:\n",
    "                wordlst.append(line)\n",
    "\n",
    "    coocurrencies = []\n",
    "\n",
    "    for listitem in wordlst:\n",
    "\n",
    "        if len(listitem) == (lengthOfItemSet + 2):\n",
    "            coocurrencies.append(listitem)\n",
    "\n",
    "    output = sorted(coocurrencies, key=lambda x: int(x[-1]))\n",
    "\n",
    "\n",
    "    for x in reversed(output):\n",
    "        if lengthOfItemSet == 2:\n",
    "            print(\"%s %s %s\" % (x[0], x[1], x[3]))\n",
    "        elif lengthOfItemSet == 3:\n",
    "            print(\"%s %s %s %s\" % (x[0], x[1], x[2], x[4]))\n",
    "        elif lengthOfItemSet == 4:\n",
    "            print(\"%s %s %s %s %s\" % (x[0], x[1], x[2], x[3], x[5]))\n",
    "        elif lengthOfItemSet == 5:\n",
    "            print(\"%s %s %s %s %s %s\" % (x[0], x[1], x[2], x[3], x[4], x[6]))\n",
    "            \n",
    "# findItemSets('FP_growth_pneumonia_alive_output.txt', 2)\n",
    "findItemSets('./tmp/111_out.txt', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
