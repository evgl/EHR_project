{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction\n",
    "Extract data from MIMIC-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import cufflinks\n",
    "\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../../datasets/mmc-3/ADMISSIONS.csv does not exist: '../../datasets/mmc-3/ADMISSIONS.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d07bc0561348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# reading csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0madmissions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../datasets/mmc-3/ADMISSIONS.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnoteevents_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../datasets/mmc-3/NOTEEVENTS.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../../datasets/mmc-3/ADMISSIONS.csv does not exist: '../../datasets/mmc-3/ADMISSIONS.csv'"
     ]
    }
   ],
   "source": [
    "# Import pandas \n",
    "import pandas as pd \n",
    "\n",
    "start = time.time()\n",
    "# reading csv file \n",
    "admissions_df = pd.read_csv(\"datasets/mmc-3/ADMISSIONS.csv\")\n",
    "noteevents_df = pd.read_csv(\"datasets/mmc-3/ADMISSIONS.csv\") \n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Left join of two dataframes\n",
    "note_admiss_df_left = noteevents_df.merge(admissions_df, on='HADM_ID', how='left', indicator=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "note_admiss_df_left[['DIAGNOSIS', 'SUBJECT_ID_x', 'SUBJECT_ID_y','DESCRIPTION', 'CATEGORY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pneumonia_df = note_admiss_df_left.loc[note_admiss_df_left[\"DIAGNOSIS\"] == 'PNEUMONIA', ['ROW_ID_x', 'SUBJECT_ID_x', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'DIAGNOSIS', 'HAS_CHARTEVENTS_DATA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pneumonia_no_disch_df = pneumonia_df.loc[pneumonia_df[\"CATEGORY\"] != 'Discharge summary', ['ROW_ID_x', 'SUBJECT_ID_x', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ADMISSION_TYPE', 'DIAGNOSIS', 'HAS_CHARTEVENTS_DATA']]\n",
    "pneumonia_no_disch_df = pneumonia_df.loc[pneumonia_df[\"CATEGORY\"] != 'Discharge summary', ['ROW_ID_x','SUBJECT_ID_x','CHARTDATE','STORETIME','CATEGORY','DESCRIPTION','TEXT', 'DEATHTIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch = pneumonia_no_disch_df[pneumonia_no_disch_df.DEATHTIME.isnull()]\n",
    "pneumonia_dead_no_disch = pneumonia_no_disch_df[pneumonia_no_disch_df.DEATHTIME.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_dead_no_disch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_alive_no_disch = pneumonia_alive_no_disch.sort_values(by=['SUBJECT_ID_x','CHARTDATE', 'ROW_ID_x'])\n",
    "pneumonia_dead_no_disch = pneumonia_dead_no_disch.sort_values(by=['SUBJECT_ID_x','CHARTDATE', 'ROW_ID_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patients\n",
    "pneumonia_dead_no_disch['SUBJECT_ID_x'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of patients\n",
    "pneumonia_alive_no_disch['SUBJECT_ID_x'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text handling tool\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "# define constants\n",
    "# RESULT_HEADER = \"WORD, FREQUENCY\\n\"\n",
    "MIN_SEQ_LEN = 4\n",
    "USE_1_N_SEQ = 2\n",
    "\n",
    "\n",
    "# words that do not have meaning (can be modified later)\n",
    "USELESS_WORDS = [\"a\", \"the\", \"he\", \"she\", \",\", \".\", \"?\", \"!\", \":\", \";\", \"+\", \"*\", \"**\"\\\n",
    "                 \"your\", \"you\"]\n",
    "\n",
    "# count up the frequency of every word in every disease file\n",
    "stemmer = PorterStemmer()\n",
    "# create set of words to ignore in text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for word in USELESS_WORDS:\n",
    "    stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------\n",
    "def count_notes_per_patient(disease_df):\n",
    "    patient_id_to_num_notes = {}\n",
    "    patient_id = -1\n",
    "    note_counter = 0\n",
    "            \n",
    "    for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "        patient_id_check = int(row['SUBJECT_ID_x'])\n",
    "                \n",
    "        if not patient_id == patient_id_check:\n",
    "            patient_id_to_num_notes[patient_id] = note_counter\n",
    "            note_counter = 1\n",
    "        else:\n",
    "            note_counter += 1\n",
    "                    \n",
    "        patient_id = patient_id_check\n",
    "                \n",
    "    patient_id_to_num_notes[patient_id] = note_counter\n",
    "    del patient_id_to_num_notes[-1]\n",
    "    return patient_id_to_num_notes\n",
    "\n",
    "patient_id_to_num_notes = {}\n",
    "patient_id_to_num_notes['pneumonia_dead'] = count_notes_per_patient(pneumonia_dead_no_disch)\n",
    "patient_id_to_num_notes['pneumonia_alive'] = count_notes_per_patient(pneumonia_alive_no_disch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_per_patient(disease_df, patient_id_to_num_notes):\n",
    "    note_appearance_counter = {}\n",
    "    number_of_patients = 0 # number of patients\n",
    "    note_counter = 0\n",
    "\n",
    "# -----------\n",
    "    patient_id = -1\n",
    "    word_set = set()\n",
    "    note_event_counter = 0\n",
    "\n",
    "    # Iterate through each note\n",
    "    for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "\n",
    "        \n",
    "        patient_id_check = int(row['SUBJECT_ID_x'])\n",
    "    \n",
    "        # if patient id has changed, end sequence and start new sequence\n",
    "        if not patient_id == patient_id_check:\n",
    "            number_of_patients += 1\n",
    "            note_event_counter = 0\n",
    "        \n",
    "            for word in word_set:\n",
    "                if word in note_appearance_counter:\n",
    "                    note_appearance_counter[word] += 1\n",
    "                else:\n",
    "                    note_appearance_counter[word] = 1\n",
    "\n",
    "        \n",
    "            # reset word_set\n",
    "            word_set = set()\n",
    "        \n",
    "        # update patient id\n",
    "        patient_id = patient_id_check\n",
    "\n",
    "            \n",
    "        if patient_id_to_num_notes[patient_id_check] <= MIN_SEQ_LEN:\n",
    "            continue\n",
    "            \n",
    "        if note_event_counter < patient_id_to_num_notes[patient_id] // USE_1_N_SEQ:\n",
    "            note_event_counter += 1\n",
    "            continue\n",
    "                \n",
    "        note_counter += 1\n",
    "        note = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|[_,\\d\\*:~=\\.\\-\\+\\\\/]+', ' ', row['TEXT'])\n",
    "        tokenized_note = word_tokenize(note)\n",
    "        \n",
    "        \n",
    "        for word in tokenized_note:\n",
    "            stemmed_word = stemmer.stem(word.lower())\n",
    "            if not stemmed_word in stop_words:\n",
    "                word_set.add(stemmed_word)\n",
    "    \n",
    "    print(str(note_counter) + \" note events\")\n",
    "    print(\"finished counting frequent words for patients!\")\n",
    "#     return note_counter, note_appearance_counter\n",
    "    return number_of_patients, note_appearance_counter\n",
    "\n",
    "# variable dictionaries\n",
    "number_of_notes = {}\n",
    "note_appearance_counter = {}\n",
    "\n",
    "number_of_notes['pneumonia_dead'], note_appearance_counter['pneumonia_dead'] = count_words_per_patient(pneumonia_dead_no_disch, patient_id_to_num_notes['pneumonia_dead'])\n",
    "number_of_notes['pneumonia_alive'], note_appearance_counter['pneumonia_alive'] = count_words_per_patient(pneumonia_alive_no_disch, patient_id_to_num_notes['pneumonia_alive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get hyper-paramters n_fold and threshold from user input\n",
    "n_fold = float(3)\n",
    "threshold = float(0.01)\n",
    "\n",
    "frequent_word_lists = {}\n",
    "factor = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function description:\n",
    "for each disease in note_appearance_counter\n",
    "    1. checks whether a word in disease file is frequent(frequency standard as defined by factor, n_fold, and threshold)\n",
    "    2. adds to frequent_word_list\n",
    "\"\"\"\n",
    "\n",
    "# def find_frequent_word(note_appearance_counter, frequent_word_lists, number_of_notes, factor, n_fold, threshold):\n",
    "\n",
    "# calculate normalizing factor for each disease\n",
    "note_sum = 0\n",
    "\n",
    "# Count from two labels\n",
    "for disease in number_of_notes:\n",
    "    note_sum += float(number_of_notes[disease])\n",
    "    \n",
    "for disease in number_of_notes:\n",
    "    factor[disease] = number_of_notes[disease] / note_sum\n",
    "\n",
    "# determine frequent word for each disease file\n",
    "for disease in note_appearance_counter:\n",
    "    frequent_word_lists[disease] = []\n",
    "\n",
    "    print(disease + \" has \" + str(len(note_appearance_counter[disease])) + \" unique words!\")\n",
    "\n",
    "    for word in note_appearance_counter[disease]:\n",
    "        \n",
    "        freq_check = True\n",
    "        for check_disease in note_appearance_counter:\n",
    "            \n",
    "            if not disease == check_disease:\n",
    "                if word in note_appearance_counter[check_disease]:\n",
    "                    if not (note_appearance_counter[disease][word] / note_appearance_counter[check_disease][word] / factor[disease] * factor[check_disease] > n_fold \\\n",
    "                        and note_appearance_counter[disease][word] > (number_of_notes[disease] * threshold)):\n",
    "\n",
    "                        freq_check = False\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    if not (note_appearance_counter[disease][word] > n_fold and note_appearance_counter[disease][word] > (number_of_notes[disease] * threshold)):\n",
    "                        freq_check = False\n",
    "                        break\n",
    "        if freq_check:\n",
    "            frequent_word_lists[disease].append((word))\n",
    "            # Create a tuple of word and its count\n",
    "#             frequent_word_lists[disease].append((word, note_appearance_counter[disease][word]))\n",
    "\n",
    "        \n",
    "\n",
    "print(\"finished making frequent words list for \" + disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frequent_word_lists['pneumonia_dead']))\n",
    "print(len(frequent_word_lists['pneumonia_alive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Co-occurrence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENT_WORD_LIST = frequent_word_lists['pneumonia_dead'] + frequent_word_lists['pneumonia_alive']\n",
    "print(len(FREQUENT_WORD_LIST))\n",
    "print(len(set(FREQUENT_WORD_LIST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"function description:\n",
    "generates frequent word set for the disease\n",
    "\"\"\"\n",
    "word_dict = {}\n",
    "word_id = 1\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "for word in FREQUENT_WORD_LIST:\n",
    "    if not word == \"WORD\":\n",
    "        word_dict[stemmer.stem(word.strip())] = word_id\n",
    "        word_id += 1\n",
    "             \n",
    "print(f\"\\nword dictionary created! Length: {len(word_dict)}\\n\")\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count co-occurrences per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparison algorithms\n",
    "# Apriori, Fpgrowth, fp_max\n",
    "\n",
    "# Apriori\n",
    "import pandas as pd\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "# ----\n",
    "def apr_patterns_per_patient_python(disease_df, min_support):\n",
    "    \n",
    "    # For dataframe\n",
    "    patient_cooc_dict = {}\n",
    "    patient_node_dict = {}\n",
    "    patient_note_cnt = {}\n",
    "    patient_itemset_dict = {}\n",
    "    \n",
    "    # --------------    \n",
    "    patient_id = -1\n",
    "    note_cnt = 0\n",
    "    patient_note_list = []\n",
    "    \n",
    "    # read line in from file (each line is one note)\n",
    "    for index, row in tqdm(disease_df.iterrows(), total=disease_df.shape[0]):\n",
    "        \n",
    "        # only regard certain type of notes\n",
    "        patient_id_check = int(row['SUBJECT_ID_x'])\n",
    "        note = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|[_,\\d\\*:~=\\.\\-\\+\\\\/]+', ' ', row['TEXT'])\n",
    "        patient_word_set = set()\n",
    "    \n",
    "#         print(f\"patient_id_check: {patient_id_check}, patient_id: {patient_id}\")\n",
    "        # if patient id has changed, end sequence and start new sequence\n",
    "        if not patient_id == patient_id_check and not patient_id == -1:\n",
    "            te = TransactionEncoder()\n",
    "            te_ary = te.fit(patient_note_list).transform(patient_note_list)\n",
    "            df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "            df_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "            \n",
    "            cooc_tmp = []\n",
    "            cooc_minsup_tmp = []\n",
    "            node_minsup_tmp = []\n",
    "            cooc_node_idx_tmp = []\n",
    "            itemsets_tmp = []\n",
    "            \n",
    "            for index, row in df_itemsets.iterrows():\n",
    "                if len(row['itemsets']) == 1:\n",
    "                    word = list(row['itemsets'])[0]\n",
    "                    min_sup = row['support']\n",
    "\n",
    "                    node_minsup_tmp.append(len(patient_note_list) * row['support'])\n",
    "                    cooc_node_idx_tmp.append(word)\n",
    "                    \n",
    "                if len(row['itemsets']) == 2:\n",
    "                    cooc_ = sorted(list(row['itemsets']))\n",
    "                    cooc_tmp.append(cooc_)\n",
    "                    cooc_minsup_tmp.append(len(patient_note_list) * row['support'])\n",
    "\n",
    "                if len(row['itemsets']) >= 2:\n",
    "                    itemset = sorted(list(row['itemsets']))\n",
    "                    itemsets_tmp.append(itemset)\n",
    "                    \n",
    "            \n",
    "            cooc_dict = {}\n",
    "\n",
    "            for num, i in enumerate(cooc_tmp):\n",
    "                if tuple(i) not in cooc_dict:\n",
    "                    cooc_dict[tuple(i)] = cooc_minsup_tmp[num]\n",
    "\n",
    "\n",
    "            node_dict = {}\n",
    "            for num, i in enumerate(cooc_node_idx_tmp):\n",
    "                if i not in node_dict:\n",
    "                    node_dict[i] = node_minsup_tmp[num]\n",
    "\n",
    "            # Update glob lists\n",
    "            if patient_id not in patient_cooc_dict:\n",
    "                patient_cooc_dict[patient_id] = cooc_dict\n",
    "                patient_node_dict[patient_id] = node_dict\n",
    "                patient_note_cnt[patient_id] = note_cnt\n",
    "                patient_itemset_dict[patient_id] = itemsets_tmp\n",
    "    \n",
    "            else:\n",
    "                print(f\"patient_id: {patient_id} is already in the dictionary!\")\n",
    "            \n",
    "            \n",
    "            # Reset local lists\n",
    "            patient_note_list = []\n",
    "            note_cnt = 0\n",
    "                    \n",
    "        # update patient id\n",
    "        patient_id = patient_id_check\n",
    "        tokenized_note = word_tokenize(note)\n",
    "        note_cnt += 1\n",
    "\n",
    "        # loop through each word in note to count word belonging to each disease\n",
    "        for word in tokenized_note:\n",
    "            stemmed_word = stemmer.stem(word.lower())       \n",
    "#             if stemmed_word in word_dict:\n",
    "            patient_word_set.add(stemmed_word)\n",
    "\n",
    "        templst = []\n",
    "        for word in patient_word_set:\n",
    "            templst.append(word)\n",
    "\n",
    "        if templst:\n",
    "            patient_note_list.append(templst)\n",
    "    \n",
    "    # Last patient info\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(patient_note_list).transform(patient_note_list)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    df_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "            \n",
    "    cooc_tmp = []\n",
    "    cooc_minsup_tmp = []\n",
    "    #-----\n",
    "    node_minsup_tmp = []\n",
    "    cooc_node_idx_tmp = []\n",
    "    itemsets_tmp = []\n",
    "            \n",
    "    for index, row in df_itemsets.iterrows():\n",
    "        if len(row['itemsets']) == 1:\n",
    "            word = list(row['itemsets'])[0]\n",
    "            node_minsup_tmp.append(len(patient_note_list) * row['support'])\n",
    "            cooc_node_idx_tmp.append(word)\n",
    "\n",
    "        if len(row['itemsets']) == 2:\n",
    "            cooc_ = sorted(list(row['itemsets']))\n",
    "            cooc_tmp.append(cooc_)\n",
    "            cooc_minsup_tmp.append(len(patient_note_list) * row['support'])\n",
    "\n",
    "        if len(row['itemsets']) >= 2:\n",
    "            itemset = sorted(list(row['itemsets']))\n",
    "            itemsets_tmp.append(itemset)\n",
    "\n",
    "    cooc_dict = {}\n",
    "\n",
    "    for num, i in enumerate(cooc_tmp):\n",
    "        if tuple(i) not in cooc_dict:\n",
    "            cooc_dict[tuple(i)] = cooc_minsup_tmp[num]\n",
    "\n",
    "\n",
    "    node_dict = {}\n",
    "    for num, i in enumerate(cooc_node_idx_tmp):\n",
    "        if i not in node_dict:\n",
    "            node_dict[i] = node_minsup_tmp[num]\n",
    "    \n",
    "                \n",
    "\n",
    "    # Update glob lists\n",
    "    if patient_id not in patient_cooc_dict:\n",
    "        patient_cooc_dict[patient_id] = cooc_dict\n",
    "        patient_node_dict[patient_id] = node_dict\n",
    "        patient_note_cnt[patient_id] = note_cnt\n",
    "        patient_itemset_dict[patient_id] = itemsets_tmp\n",
    "    else:\n",
    "        print(f\"patient_id: {patient_id} is already in the dictionary!\")\n",
    "\n",
    "    return patient_node_dict, patient_cooc_dict, patient_itemset_dict, patient_note_cnt\n",
    "\n",
    "apr_patient_node_0, apr_patient_cooc_0, apr_patient_itemset_0, apr_patient_note_num_0 = apr_patterns_per_patient_python(pneumonia_dead_no_disch, 0.999)\n",
    "apr_patient_node_1, apr_patient_cooc_1, apr_patient_itemset_1, apr_patient_note_num_1 = apr_patterns_per_patient_python(pneumonia_alive_no_disch, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all unique co-occurrences from both group\n",
    "patient_cooc_set = set()\n",
    "patient_cooc_0_dict = {}\n",
    "patient_cooc_1_dict = {}\n",
    "\n",
    "for k, v in apr_patient_cooc_0.items():\n",
    "    for item in v:\n",
    "        patient_cooc_set.add(item)\n",
    "        if item not in patient_cooc_0_dict:\n",
    "            patient_cooc_0_dict[item] = v[item]\n",
    "        else:\n",
    "            patient_cooc_0_dict[item] = patient_cooc_0_dict[item] + v[item]\n",
    "            \n",
    "    \n",
    "for k, v in apr_patient_cooc_1.items():\n",
    "    for item in v:\n",
    "        patient_cooc_set.add(item)\n",
    "        if item not in patient_cooc_1_dict:\n",
    "            patient_cooc_1_dict[item] = v[item]\n",
    "        else:\n",
    "            patient_cooc_1_dict[item] = patient_cooc_1_dict[item] + v[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(patient_cooc_set))\n",
    "print(len(patient_cooc_0_dict))\n",
    "print(len(patient_cooc_1_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "patient_cooc_odd_scores = {}\n",
    "for set_item in patient_cooc_set:\n",
    "    if set_item in patient_cooc_0_dict and set_item in patient_cooc_1_dict: \n",
    "        d_prob = patient_cooc_0_dict[set_item]/(patient_cooc_0_dict[set_item] + patient_cooc_1_dict[set_item])\n",
    "        a_prob = patient_cooc_1_dict[set_item]/(patient_cooc_0_dict[set_item] + patient_cooc_1_dict[set_item])\n",
    "        log_odd_score = math.log((a_prob + 0.001)/(d_prob+0.001))\n",
    "        patient_cooc_odd_scores[set_item] = log_odd_score\n",
    "    elif set_item in patient_cooc_0_dict:\n",
    "        log_odd_score = math.log((0.001)/(1.001))\n",
    "        patient_cooc_odd_scores[set_item] = log_odd_score\n",
    "    elif set_item in patient_cooc_1_dict:\n",
    "        log_odd_score = math.log((1.001)/(0.001))\n",
    "        patient_cooc_odd_scores[set_item] = log_odd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_cooc_odd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "import numpy\n",
    "def data_norm(cooc_odd_scores):\n",
    "\n",
    "    def norm_arr(array):\n",
    "        arr = numpy.array(list(array))\n",
    "        start = 0\n",
    "        end = 1\n",
    "        width = end - start\n",
    "        res = (arr - arr.min())/(arr.max() - arr.min()) * width + start\n",
    "        return res.tolist()\n",
    "\n",
    "    cooc_keys, cooc_values = zip(*cooc_odd_scores.items())\n",
    "    new_cooc_odd_scores = dict(zip(cooc_keys, norm_arr(cooc_values)))\n",
    "\n",
    "    return new_cooc_odd_scores\n",
    "\n",
    "new_patient_cooc_odd_scores = data_norm(patient_cooc_odd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code to label node words\n",
    "apr_patient_node_set = set()\n",
    "apr_patient_node_0_dict = {}\n",
    "apr_patient_node_1_dict = {}\n",
    "    \n",
    "for k, v in apr_patient_node_0.items():\n",
    "    for item in v:\n",
    "        apr_patient_node_set.add(item)\n",
    "        \n",
    "        if item not in apr_patient_node_0_dict:\n",
    "            apr_patient_node_0_dict[item] = v[item]\n",
    "        else:\n",
    "            apr_patient_node_0_dict[item] = apr_patient_node_0_dict[item] + v[item]\n",
    "        \n",
    "    \n",
    "for k, v in apr_patient_node_1.items():\n",
    "    for item in v:\n",
    "        apr_patient_node_set.add(item)\n",
    "        \n",
    "        if item not in apr_patient_node_1_dict:\n",
    "            apr_patient_node_1_dict[item] = v[item]\n",
    "        else:\n",
    "            apr_patient_node_1_dict[item] = apr_patient_node_1_dict[item] + v[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(apr_patient_node_set))\n",
    "print(len(apr_patient_node_0_dict))\n",
    "print(len(apr_patient_node_1_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_patient_node_odd_scores = {}\n",
    "\n",
    "for set_item in apr_patient_node_set:\n",
    "    if set_item in apr_patient_node_0_dict and set_item in apr_patient_node_1_dict:\n",
    "        d_prob = apr_patient_node_0_dict[set_item]/(apr_patient_node_0_dict[set_item] + apr_patient_node_1_dict[set_item])\n",
    "        a_prob = apr_patient_node_1_dict[set_item]/(apr_patient_node_0_dict[set_item] + apr_patient_node_1_dict[set_item])\n",
    "        log_odd_score = math.log((a_prob + 0.001)/(d_prob+0.001))\n",
    "        apr_patient_node_odd_scores[set_item] = log_odd_score\n",
    "    elif set_item in apr_patient_node_0_dict:\n",
    "        log_odd_score = math.log((0.001)/(1.001))\n",
    "        apr_patient_node_odd_scores[set_item] = log_odd_score\n",
    "    elif set_item in apr_patient_node_1_dict:\n",
    "        log_odd_score = math.log((1.001)/(0.001))\n",
    "        apr_patient_node_odd_scores[set_item] = log_odd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(apr_patient_node_odd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_patient_square_node_data = pd.DataFrame({'node':list(apr_patient_node_odd_scores.keys()), 'feature':list(apr_patient_node_odd_scores.values())})\n",
    "apr_patient_square_node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_patient_square_node_id_data = apr_patient_square_node_data.set_index(\"node\")\n",
    "apr_patient_square_node_id_data['subject'] = [\"positive\" if r['feature'] > 0 else \"negative\" for i, r in apr_patient_square_node_id_data.iterrows()]\n",
    "apr_patient_square_node_id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_subjects = apr_patient_square_node_id_data[\"subject\"]\n",
    "apr_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of all itemsets\n",
    "apr_itemsets = []\n",
    "\n",
    "for k,v in tqdm(apr_patient_itemset_0.items()):\n",
    "    \n",
    "    for i in v:  \n",
    "        if i not in apr_itemsets:\n",
    "            apr_itemsets.append(i)\n",
    "    \n",
    "for k,v in tqdm(apr_patient_itemset_1.items()):\n",
    "    for i in v:\n",
    "        if i not in apr_itemsets:\n",
    "            apr_itemsets.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(apr_itemsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(apr_itemsets[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embeddings from apr_cooc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_cooc_model_patient = Word2Vec(\n",
    "    apr_itemsets, size=128, window=5, min_count=0, sg=1, workers=4, iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb = apr_cooc_model_patient.wv[\"cmo\"]\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise node embeddings for apr_cooc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "patient_node_ids = apr_cooc_model_patient.wv.index2word  # list of node IDs\n",
    "patient_weighted_node_embeddings = (\n",
    "    apr_cooc_model_patient.wv.vectors\n",
    ")  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "# the gensim ordering may not match the StellarGraph one, so rearrange\n",
    "patient_node_targets = apr_subjects.loc[patient_node_ids].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE transformation on node embeddings\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "patient_weighted_node_embeddings_2d = tsne.fit_transform(patient_weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the points\n",
    "alpha = 0.7\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    patient_weighted_node_embeddings_2d[:, 0],\n",
    "    patient_weighted_node_embeddings_2d[:, 1],\n",
    "    c=patient_node_targets.cat.codes,\n",
    "    cmap=\"jet\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random vector for words that are not in a dictionary\n",
    "apr_node_emb_dict = {}\n",
    "rand_vec_cnt = 0\n",
    "for index, row in apr_patient_square_node_id_data.iterrows():\n",
    "    try:\n",
    "        if index not in apr_node_emb_dict:\n",
    "            apr_node_emb_dict[index] = apr_cooc_model_patient.wv[index]\n",
    "        else:\n",
    "            print(f\"index: {index} is already in a dictionary!\")\n",
    "    except KeyError:\n",
    "        rand_vec_cnt += 1\n",
    "        apr_node_emb_dict[index] = np.random.normal(scale=0.6, size=128)\n",
    "\n",
    "print(f\"random vector counter is {rand_vec_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import GCNSupervisedGraphClassification\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from stellargraph import datasets\n",
    "\n",
    "from sklearn import model_selection\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "sg.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_apr_graph_list(patient_cooc_dict, cooc_odd_scores, node_emb_dict, label):\n",
    "    \n",
    "    graphs = []\n",
    "    labels = []\n",
    "    \n",
    "    for key,row in patient_cooc_dict.items():\n",
    "     \n",
    "        if row:\n",
    "            source = []\n",
    "            target = []\n",
    "            # edge_weight = []\n",
    "            \n",
    "            node_feature = []\n",
    "            node_idx = []\n",
    "            for cooc in row:\n",
    "                source.extend([cooc[0], cooc[1]])\n",
    "                target.extend([cooc[1], cooc[0]])\n",
    "                # edge_weight.extend([cooc_odd_scores[cooc], cooc_odd_scores[cooc]])\n",
    "        \n",
    "            node_idx = list(set(source + target))\n",
    "    \n",
    "            # Create a dataframe of only nodes\n",
    "            square_node_data = pd.DataFrame(\n",
    "                index=node_idx)\n",
    "            \n",
    "#             square_node_data['feature'] = [1] * len(node_idx)\n",
    "            \n",
    "            # Create a dictionary for each column for a vector\n",
    "            node_features = defaultdict(list)\n",
    "            for node in node_idx:\n",
    "                for i, vec in enumerate(node_emb_dict[node]):\n",
    "                    node_features['w_' + str(i)].append(vec)\n",
    "        \n",
    "            # Add columns to a dataframe\n",
    "            for k, v in node_features.items():\n",
    "              \n",
    "                square_node_data[k] = v\n",
    "\n",
    "            square_edges = pd.DataFrame({ \n",
    "                \"source\": source, \n",
    "                \"target\": target, \n",
    "                # \"weight\":edge_weight\n",
    "            })\n",
    "        \n",
    "            square = StellarGraph({\"corner\": square_node_data}, {\"line\": square_edges})\n",
    "            graphs.append(square)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return graphs, labels\n",
    "\n",
    "graphs = []\n",
    "labels = []\n",
    "\n",
    "# patient_weighted_node_emb_dict\n",
    "graph_0, label_0 = create_apr_graph_list(apr_patient_cooc_0, new_patient_cooc_odd_scores, apr_node_emb_dict, -1)\n",
    "graph_1, label_1 = create_apr_graph_list(apr_patient_cooc_1, new_patient_cooc_odd_scores, apr_node_emb_dict, 1)\n",
    "\n",
    "graphs.extend(graph_0)\n",
    "labels.extend(label_0)\n",
    "print(f\"graphs_0: {len(graphs)}, labels_0: {len(labels)}\")\n",
    "graphs.extend(graph_1)\n",
    "labels.extend(label_1)\n",
    "print(f\"graphs_1: {len(graph_1)}, labels_1: {len(label_1)}\")\n",
    "print(f\"graphs: {len(graphs)}, labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test and train datasets\n",
    "test_cnt = int(len(graphs)*0.1)/2\n",
    "print(test_cnt)\n",
    "\n",
    "pos_start = len(graph_0)\n",
    "print(pos_start)\n",
    "\n",
    "test_arr = []\n",
    "train_arr = []\n",
    "for i, v in enumerate(graphs):\n",
    "    # Take first items for neg set\n",
    "    if i < test_cnt:\n",
    "        test_arr.append(i)    \n",
    "    elif i > pos_start and i <= pos_start + test_cnt:\n",
    "        test_arr.append(i)    \n",
    "    else:\n",
    "        train_arr.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 42\n",
    "c = list(test_arr)\n",
    "random.Random(seed).shuffle(c)\n",
    "test_arr =  c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(train_arr)\n",
    "random.Random(seed).shuffle(c)\n",
    "train_arr =  c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.array(train_arr)\n",
    "test_index = np.array(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(test_index))\n",
    "print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(train_index))\n",
    "print(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shuffle the data\n",
    "# import random\n",
    "# seed = 42\n",
    "\n",
    "# c = list(zip(graphs, labels))\n",
    "# random.Random(seed).shuffle(c)\n",
    "\n",
    "# graphs, labels = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graphs[175].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graphs[0].node_features())\n",
    "print(len(graphs[0].node_features()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graphs[100].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(\n",
    "    [(g.number_of_nodes(), g.number_of_edges()) for g in graphs],\n",
    "    columns=[\"nodes\", \"edges\"],\n",
    ")\n",
    "summary.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels = pd.Series(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels = pd.get_dummies(graph_labels, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(graph_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = PaddedGraphGenerator(graphs=graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_classification_model(generator):\n",
    "    gc_model = GCNSupervisedGraphClassification(\n",
    "        layer_sizes=[64, 64],\n",
    "        activations=[\"relu\", \"relu\"],\n",
    "        generator=generator,\n",
    "        dropout=0.5,\n",
    "    )\n",
    "    x_inp, x_out = gc_model.in_out_tensors()\n",
    "    predictions = Dense(units=32, activation=\"relu\")(x_out)\n",
    "    predictions = Dense(units=16, activation=\"relu\")(predictions)\n",
    "    predictions = Dense(units=1, activation=\"sigmoid\")(predictions)\n",
    "\n",
    "    # Let's create the Keras model and prepare it for training\n",
    "    model = Model(inputs=x_inp, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(0.005), loss=binary_crossentropy, metrics=[\"acc\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200  # maximum number of training epochs\n",
    "# folds = 10  # the number of folds for k-fold cross validation\n",
    "# n_repeats = 5  # the number of repeats for repeated k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=25, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train in folds\n",
    "def train_fold(model, train_gen, test_gen, es, epochs):\n",
    "    history = model.fit(\n",
    "        train_gen, epochs=epochs, validation_data=test_gen, verbose=0, callbacks=[es],\n",
    "    )\n",
    "    # calculate performance on the test data and return along with history\n",
    "    test_metrics = model.evaluate(test_gen, verbose=0)\n",
    "    test_acc = test_metrics[model.metrics_names.index(\"acc\")]\n",
    "\n",
    "    return history, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train in folds\n",
    "def get_generators(train_index, test_index, graph_labels, batch_size):\n",
    "    train_gen = generator.flow(\n",
    "        train_index, targets=graph_labels.iloc[train_index].values, weighted=True, batch_size=batch_size, shuffle=False, seed=42\n",
    "    )\n",
    "    test_gen = generator.flow(\n",
    "        test_index, targets=graph_labels.iloc[test_index].values, weighted=True, batch_size=batch_size, shuffle=False, seed=42\n",
    "    )\n",
    "\n",
    "    return train_gen, test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # To train in folds\n",
    "# test_accs = []\n",
    "\n",
    "# stratified_folds = model_selection.RepeatedStratifiedKFold(\n",
    "#     n_splits=folds, n_repeats=n_repeats\n",
    "# ).split(graph_labels, graph_labels)\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "#     print(f\"Training and evaluating on fold {i+1} out of {folds * n_repeats}...\")\n",
    "#     train_gen, test_gen = get_generators(\n",
    "#         train_index, test_index, graph_labels, batch_size=30\n",
    "#     )\n",
    "\n",
    "#     model = create_graph_classification_model(generator)\n",
    "\n",
    "#     history, acc = train_fold(model, train_gen, test_gen, es, epochs)\n",
    "\n",
    "#     test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To train in folds\n",
    "test_accs = []\n",
    "\n",
    "# stratified_folds = model_selection.RepeatedStratifiedKFold(\n",
    "#     n_splits=folds, n_repeats=n_repeats\n",
    "# ).split(graph_labels, graph_labels)\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "for i in range(50):\n",
    "    print(f\"Training and evaluating on fold {i+1}...\")\n",
    "    \n",
    "    train_gen, test_gen = get_generators(\n",
    "        train_index, test_index, graph_labels, batch_size=30\n",
    "    )\n",
    "\n",
    "    model = create_graph_classification_model(generator)\n",
    "\n",
    "    history, acc = train_fold(model, train_gen, test_gen, es, epochs)\n",
    "\n",
    "    test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Accuracy over all folds mean: 76.0% and std: 1.8%\n",
    "# 2: Accuracy over all folds mean: 75.7% and std: 4.7%\n",
    "# 3: Accuracy over all folds mean: 73.8% and std: 1.9%\n",
    "# 4: Accuracy over all folds mean: 76.2% and std: 4.2%\n",
    "# 5: Accuracy over all folds mean: 70.3% and std: 5.3%\n",
    "# 6: Accuracy over all folds mean: 74.0% and std: 5.2%\n",
    "\n",
    "#---------- After setting seed\n",
    "# 1: Accuracy over all folds mean: 73.5% and std: 6.7%\n",
    "# 2: Accuracy over all folds mean: 73.3% and std: 3.9%\n",
    "# 3: Accuracy over all folds mean: 74.7% and std: 3.9%\n",
    "# 4: Accuracy over all folds mean: 74.4% and std: 2.0%\n",
    "# 5: Accuracy over all folds mean: 74.2% and std: 1.6%\n",
    "# 6: Accuracy over all folds mean: 74.3% and std: 2.4%\n",
    "\n",
    "# Sepsis\n",
    "# 1: Accuracy over all folds mean: 76.3% and std: 2.4%\n",
    "print(\n",
    "    f\"Accuracy over all folds mean: {np.mean(test_accs)*100:.3}% and std: {np.std(test_accs)*100:.2}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(test_accs)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd try\n",
    "print(\n",
    "    f\"Accuracy over all folds mean: {np.mean(test_accs)*100:.3}% and std: {np.std(test_accs)*100:.2}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(test_accs)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # shuffle the data\n",
    "# import random\n",
    "\n",
    "# c = list(zip(graphs, labels))\n",
    "# random.shuffle(c)\n",
    "\n",
    "# graphs, labels = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stratified_folds = model_selection.RepeatedStratifiedKFold(\n",
    "#     n_splits=10, n_repeats=5\n",
    "# ).split(graph_labels, graph_labels)\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "#     print(f\"\\ntrain_index: \\n{train_index}\\ntest_index: \\n{test_index}\\n\")\n",
    "#     break\n",
    "# print(f\"train: {len(train_index)}, test: {len(test_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(graph_labels.iloc[train_index].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_labels.iloc[test_index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(\n",
    "        train_index, targets=graph_labels.iloc[train_index].values, batch_size=40, shuffle=False, seed=42)\n",
    "test_gen = generator.flow(\n",
    "    test_index, targets=graph_labels.iloc[test_index].values, batch_size=40, shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_graph_classification_model(generator)\n",
    "\n",
    "history = model.fit(\n",
    "        train_gen, epochs=200, validation_data=test_gen, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate performance on the test data and return along with history\n",
    "# Node features all 1, edge features all 1\n",
    "test_metrics = model.evaluate(test_gen, verbose=1)\n",
    "test_acc = test_metrics[model.metrics_names.index(\"acc\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_weighted\n",
    "print(test_metrics)\n",
    "#test loss, test acc: [0.5977194905281067, 0.6794871687889099]\n",
    "# 2nd try: [0.6099531650543213, 0.7435897588729858]\n",
    "# 3: [0.6134859919548035, 0.692307710647583]\n",
    "# 4: [0.5708641409873962, 0.7564102411270142]\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Generate predictions for samples\")\n",
    "predictions = model.predict(test_gen, verbose=0)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "# predictions shape: (79, 1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.9633641 ]\n",
    " [0.9572478 ]\n",
    " [0.92709166]\n",
    " [0.67149305]\n",
    " [0.75392646]\n",
    " [0.80909836]\n",
    " [0.8878052 ]\n",
    " [0.35882965]\n",
    " [0.8740735 ]\n",
    " [0.9488548 ]\n",
    " [0.71857995]\n",
    " [0.8417202 ]\n",
    " [0.77317935]\n",
    " [0.97507536]\n",
    " [0.7930052 ]\n",
    " [0.3096424 ]\n",
    " [0.81946105]\n",
    " [0.3233545 ]\n",
    " [0.75175   ]\n",
    " [0.79594046]\n",
    " [0.2881279 ]\n",
    " [0.42757368]\n",
    " [0.35783005]\n",
    " [0.53080255]\n",
    " [0.42272243]\n",
    " [0.8731016 ]\n",
    " [0.2651747 ]\n",
    " [0.9176288 ]\n",
    " [0.8412247 ]\n",
    " [0.84611094]\n",
    " [0.899879  ]\n",
    " [0.4730718 ]\n",
    " [0.6183011 ]\n",
    " [0.8530265 ]\n",
    " [0.38916534]\n",
    " [0.41842264]\n",
    " [0.90864706]\n",
    " [0.82059276]\n",
    " [0.89827585]\n",
    " [0.31872994]\n",
    " [0.9104121 ]\n",
    " [0.8878981 ]\n",
    " [0.4407365 ]\n",
    " [0.6909543 ]\n",
    " [0.75392646]\n",
    " [0.88234955]\n",
    " [0.75392646]\n",
    " [0.75392646]\n",
    " [0.2803172 ]\n",
    " [0.8783453 ]\n",
    " [0.6334169 ]\n",
    " [0.8953688 ]\n",
    " [0.9676844 ]\n",
    " [0.46171212]\n",
    " [0.42416874]\n",
    " [0.7234482 ]\n",
    " [0.93954104]\n",
    " [0.7739153 ]\n",
    " [0.37732255]\n",
    " [0.7243462 ]\n",
    " [0.975207  ]\n",
    " [0.39652586]\n",
    " [0.6909691 ]\n",
    " [0.75392646]\n",
    " [0.3582555 ]\n",
    " [0.75392646]\n",
    " [0.94787574]\n",
    " [0.47389334]\n",
    " [0.9733413 ]\n",
    " [0.75392646]\n",
    " [0.931447  ]\n",
    " [0.53550804]\n",
    " [0.31291372]\n",
    " [0.3546905 ]\n",
    " [0.75392646]\n",
    " [0.75392646]\n",
    " [0.3483727 ]\n",
    " [0.7502146 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_gen.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = np.array(test_gen.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_test = np.argmax(temp_test, axis=1) # Convert one-hot to index\n",
    "y_pred = model.predict(test_gen)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
